{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b9dc4ed-136e-4e01-9029-edf637d1dea6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "# Load Parquet\n",
    "df = spark.read.csv(\"/Volumes/workspace/default/delta_quality/delta_quality_input.csv\", header=True, inferSchema=True)\n",
    "df.show()\n",
    "\n",
    "# Save as Delta Table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"data_quality_file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d39e747a-0c07-469e-aa05-f0ea292b2837",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **Create Data Quality Session Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65412eca-578e-48e3-abb5-8073a8751346",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "\n",
    "# Create schema if not exists\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS data_quality_file\")\n",
    "\n",
    "# Create session table if not exists\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS data_quality_file_session_log (\n",
    "    session_id STRING,\n",
    "    start_time TIMESTAMP,\n",
    "    end_time TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# Generate session details\n",
    "session_id = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"session_id\", StringType(), True),\n",
    "    StructField(\"start_time\", TimestampType(), True),\n",
    "    StructField(\"end_time\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "session_df = spark.createDataFrame([(\n",
    "    session_id,\n",
    "    start_time,\n",
    "    None  # null end time\n",
    ")], schema=schema)\n",
    "\n",
    "# Append new session\n",
    "session_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"data_quality_file_session_log\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41a2f0e9-601b-4096-8652-163e2fa49c27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from data_quality_file_session_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e42a9091-8c02-456c-8194-aa7359034c37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ** Data Quality Summary Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3d97750-992b-4056-849d-85bb6b399fad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure schema exists before saving\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS data_summary_file\")\n",
    "\n",
    "# Get column names\n",
    "columns = df.columns\n",
    "total_rows = df.count()\n",
    "\n",
    "# Build summary rows\n",
    "summary_data = []\n",
    "file_name = \"sales_data\"  # Use real file name if available\n",
    "\n",
    "for col_name in columns:\n",
    "    null_count = df.filter(col(col_name).isNull()).count()\n",
    "    \n",
    "    summary_data.append({\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"session_id\": session_id,\n",
    "        \"file_name\": file_name,\n",
    "        \"column_name\": col_name,\n",
    "        \"total_rows\": total_rows,\n",
    "        \"null_failed\": null_count\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "dq_summary_df = spark.createDataFrame(summary_data)\n",
    "\n",
    "# Save summary\n",
    "dq_summary_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"data_summary_file.dq_summary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91537ec1-2368-45e1-98de-05893c7144ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from data_summary_file.dq_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "803bac5f-62d8-4a5b-a706-b628b4b6017d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **Update Session End Time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbb1a984-dea5-4937-b1a1-5239539918a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, lit\n",
    "from datetime import datetime\n",
    "\n",
    "# Update end time\n",
    "end_time = datetime.now()\n",
    "\n",
    "# Load and update session\n",
    "df_sess = spark.table(\"delta_summary_file.dq_summary\") \\\n",
    "    .withColumn(\"end_time\", when(col(\"session_id\") == session_id, lit(end_time)))\n",
    "\n",
    "df_sess.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").saveAsTable(\"delta_summary_file.dq_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a6cbed7-d134-48d3-a56c-8cb31701de9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select * from data_summary_file.dq_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3970a17-6e5a-4ff6-af0d-5b4d927e334c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **PIT Data Quality Summary **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80bbd88c-c916-4b30-b7e7-87791ed12bda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS delta_summary_file\")\n",
    "\n",
    "# Create empty PIT summary table if not exists\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS delta_summary_file.dq_pit_summary (\n",
    "    session_id STRING,\n",
    "    total_columns INT,\n",
    "    total_nulls INT,\n",
    "    quality_score DOUBLE\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "spark.sql(\"SELECT * FROM delta_summary_file.dq_pit_summary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62240f2b-0874-4d2d-a631-2b53f9be9279",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4833888958873820,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) Data_quality 2025-08-07 15:00:53",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
